{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Intent_Classification_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f05b76c973f249519c510b7aade21510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9250a549d7fd40ad93d2cdd12fb6f91f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad81993d700646b497f8fbcf2423fe15",
              "IPY_MODEL_cb136730dc874ddb88ff884956c373c8"
            ]
          }
        },
        "9250a549d7fd40ad93d2cdd12fb6f91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad81993d700646b497f8fbcf2423fe15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af99d4addbca4e56970fb2a84d59ed74",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4aa805f33c044f58c4d0dc5611364e1"
          }
        },
        "cb136730dc874ddb88ff884956c373c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29ab35ebeb04449c97bfff988caaeb57",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 990kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f209950dc361472b88dca6a52006ea59"
          }
        },
        "af99d4addbca4e56970fb2a84d59ed74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4aa805f33c044f58c4d0dc5611364e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29ab35ebeb04449c97bfff988caaeb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f209950dc361472b88dca6a52006ea59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monokim/BERT/blob/master/Intent_Classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdw6bkfeAYn0",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l3MqPxeAYn3",
        "colab_type": "text"
      },
      "source": [
        "Check If there is a GPU available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1U_spRMAYn4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cfabfbf4-f3d4-45bb-8a18-26ce6be37dcd"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print(torch.cuda.get_device_name(0), 'will be used.')\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Tesla P4 will be used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxVfEMVkAYn8",
        "colab_type": "text"
      },
      "source": [
        "IMDB Movie review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS9mqU28FZU8",
        "colab_type": "code",
        "outputId": "5f55c4b5-cdd4-4747-9b7c-1e995fdf31df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data set into a pandas dataframe\n",
        "df = pd.read_csv(\"./Dataset/IMDB_Dataset.csv\", delimiter=',', header=None, names=['review', 'sentiment'])\n",
        "\n",
        "df.sample(10)\n",
        "\n",
        "# Print number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "sentences = df.review.values[1000:]\n",
        "labels = df.sentiment.values[1000:]\n",
        "labels = [1 if l == 'positive' else 0 for l in labels]\n",
        "\n",
        "print(sentences[:3])\n",
        "print(labels[:3])\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport pandas as pd\\n\\n# Load the data set into a pandas dataframe\\ndf = pd.read_csv(\"./Dataset/IMDB_Dataset.csv\", delimiter=\\',\\', header=None, names=[\\'review\\', \\'sentiment\\'])\\n\\ndf.sample(10)\\n\\n# Print number of sentences.\\nprint(\\'Number of training sentences: {:,}\\n\\'.format(df.shape[0]))\\nsentences = df.review.values[1000:]\\nlabels = df.sentiment.values[1000:]\\nlabels = [1 if l == \\'positive\\' else 0 for l in labels]\\n\\nprint(sentences[:3])\\nprint(labels[:3])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v5awyN2bNTG",
        "colab_type": "text"
      },
      "source": [
        "AG NEWS Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU3JutbJbNnU",
        "colab_type": "code",
        "outputId": "a5a5c243-a322-4ac0-c10c-fce8a03fc674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data set into a pandas dataframe\n",
        "df = pd.read_csv(\"./Dataset/ag_news_train.csv\", delimiter=',', header=None, names=['category', \"head\", 'content'])\n",
        "\n",
        "df.sample(10)\n",
        "\n",
        "# Print number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "sentences = df.content.values\n",
        "labels = df.category.values - 1\n",
        "\n",
        "print(sentences[:3])\n",
        "print(labels[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 120,000\n",
            "\n",
            "[\"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"\n",
            " 'Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.'\n",
            " 'Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.']\n",
            "[2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ2Sv3TkAYoG",
        "colab_type": "text"
      },
      "source": [
        "# BERT Tokenizer\n",
        "\n",
        "Tokenize each words and convert to token IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiMqEtkd3T7o",
        "colab_type": "code",
        "outputId": "41a14a97-2ec1-46be-e7e4-8fa1e27529c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "# Install transformers by using pip\n",
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 28.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=018c9cce0be14d94b9f5d781cbb7f49055c1c12aea968dd208e9269cba72ec4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrqOOw-LAYoH",
        "colab_type": "code",
        "outputId": "0359a748-9e24-41e4-ac00-f3aa19c373c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "f05b76c973f249519c510b7aade21510",
            "9250a549d7fd40ad93d2cdd12fb6f91f",
            "ad81993d700646b497f8fbcf2423fe15",
            "cb136730dc874ddb88ff884956c373c8",
            "af99d4addbca4e56970fb2a84d59ed74",
            "a4aa805f33c044f58c4d0dc5611364e1",
            "29ab35ebeb04449c97bfff988caaeb57",
            "f209950dc361472b88dca6a52006ea59"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "print('Original : ', sentences[0])\n",
        "print('Tokenized : ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs : ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f05b76c973f249519c510b7aade21510",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original :  Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "Tokenized :  ['reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', '\\\\', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.']\n",
            "Token IDs :  [26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 1032, 2316, 1997, 11087, 1011, 22330, 8713, 2015, 1010, 2024, 3773, 2665, 2153, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL40H1W1AYoK",
        "colab_type": "text"
      },
      "source": [
        "Sentence to ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2m4kI1kAYoL",
        "colab_type": "code",
        "outputId": "c8d3e81e-d7bb-403d-a537-b143c4ee0a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for s in sentences:\n",
        "    encoded_sentence = tokenizer.encode(\n",
        "        s,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    input_ids.append(encoded_sentence)\n",
        "    \n",
        "print('original: ', sentences[0])\n",
        "print('id: ', input_ids[0])\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:  Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "id:  [101, 26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 1032, 2316, 1997, 11087, 1011, 22330, 8713, 2015, 1010, 2024, 3773, 2665, 2153, 1012, 102]\n",
            "Max sentence length:  372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHGEfJhAYoN",
        "colab_type": "text"
      },
      "source": [
        "Add padding and attention masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvnwuYlZAYoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_padding_and_truncate(input_ids):\n",
        "    MAX_LEN = 64\n",
        "    for index, input_id in enumerate(input_ids):\n",
        "        for i in range(MAX_LEN - len(input_id)):\n",
        "          input_id.insert(0, 0)\n",
        "        if len(input_id) > MAX_LEN:\n",
        "          input_ids[index] = input_id[:MAX_LEN]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHoWz1rtAYoQ",
        "colab_type": "code",
        "outputId": "24bbf0fa-3338-40ef-a254-705981638e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Fit sentence's length to MAX_LEN\n",
        "add_padding_and_truncate(input_ids)\n",
        "\n",
        "print('After max question length: ', max([len(id) for id in input_ids]))\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for id in input_ids:\n",
        "    att_mask = [int(token_id) > 0 for token_id in id]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After max question length:  64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca1FnQVUAYoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.1)\n",
        "train_masks, valid_masks, _, _ = train_test_split(attention_masks, labels, random_state=2020, test_size=0.1)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "valid_inputs = torch.tensor(valid_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "valid_labels = torch.tensor(valid_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "valid_masks = torch.tensor(valid_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YeciXKeAYoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
        "valid_sampler = RandomSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyvHK7gbAYoZ",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpfD-vUfAYoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 4,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "#model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "if device.type == 'cuda':\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fVa1ATgAYod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOPm2ntbAYof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlsy2adMAYoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kJa6xacAYok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ErScfHqAYom",
        "colab_type": "code",
        "outputId": "e88b3a6c-bfbe-4dea-a964-23759d4f2d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  3,375.    Elapsed: 0:00:17.\n",
            "  Batch    80  of  3,375.    Elapsed: 0:00:34.\n",
            "  Batch   120  of  3,375.    Elapsed: 0:00:50.\n",
            "  Batch   160  of  3,375.    Elapsed: 0:01:07.\n",
            "  Batch   200  of  3,375.    Elapsed: 0:01:24.\n",
            "  Batch   240  of  3,375.    Elapsed: 0:01:41.\n",
            "  Batch   280  of  3,375.    Elapsed: 0:01:59.\n",
            "  Batch   320  of  3,375.    Elapsed: 0:02:16.\n",
            "  Batch   360  of  3,375.    Elapsed: 0:02:33.\n",
            "  Batch   400  of  3,375.    Elapsed: 0:02:50.\n",
            "  Batch   440  of  3,375.    Elapsed: 0:03:08.\n",
            "  Batch   480  of  3,375.    Elapsed: 0:03:25.\n",
            "  Batch   520  of  3,375.    Elapsed: 0:03:43.\n",
            "  Batch   560  of  3,375.    Elapsed: 0:04:00.\n",
            "  Batch   600  of  3,375.    Elapsed: 0:04:17.\n",
            "  Batch   640  of  3,375.    Elapsed: 0:04:35.\n",
            "  Batch   680  of  3,375.    Elapsed: 0:04:52.\n",
            "  Batch   720  of  3,375.    Elapsed: 0:05:10.\n",
            "  Batch   760  of  3,375.    Elapsed: 0:05:27.\n",
            "  Batch   800  of  3,375.    Elapsed: 0:05:44.\n",
            "  Batch   840  of  3,375.    Elapsed: 0:06:02.\n",
            "  Batch   880  of  3,375.    Elapsed: 0:06:19.\n",
            "  Batch   920  of  3,375.    Elapsed: 0:06:37.\n",
            "  Batch   960  of  3,375.    Elapsed: 0:06:54.\n",
            "  Batch 1,000  of  3,375.    Elapsed: 0:07:12.\n",
            "  Batch 1,040  of  3,375.    Elapsed: 0:07:29.\n",
            "  Batch 1,080  of  3,375.    Elapsed: 0:07:46.\n",
            "  Batch 1,120  of  3,375.    Elapsed: 0:08:04.\n",
            "  Batch 1,160  of  3,375.    Elapsed: 0:08:21.\n",
            "  Batch 1,200  of  3,375.    Elapsed: 0:08:39.\n",
            "  Batch 1,240  of  3,375.    Elapsed: 0:08:56.\n",
            "  Batch 1,280  of  3,375.    Elapsed: 0:09:14.\n",
            "  Batch 1,320  of  3,375.    Elapsed: 0:09:31.\n",
            "  Batch 1,360  of  3,375.    Elapsed: 0:09:49.\n",
            "  Batch 1,400  of  3,375.    Elapsed: 0:10:06.\n",
            "  Batch 1,440  of  3,375.    Elapsed: 0:10:24.\n",
            "  Batch 1,480  of  3,375.    Elapsed: 0:10:41.\n",
            "  Batch 1,520  of  3,375.    Elapsed: 0:10:59.\n",
            "  Batch 1,560  of  3,375.    Elapsed: 0:11:16.\n",
            "  Batch 1,600  of  3,375.    Elapsed: 0:11:33.\n",
            "  Batch 1,640  of  3,375.    Elapsed: 0:11:51.\n",
            "  Batch 1,680  of  3,375.    Elapsed: 0:12:08.\n",
            "  Batch 1,720  of  3,375.    Elapsed: 0:12:26.\n",
            "  Batch 1,760  of  3,375.    Elapsed: 0:12:43.\n",
            "  Batch 1,800  of  3,375.    Elapsed: 0:13:01.\n",
            "  Batch 1,840  of  3,375.    Elapsed: 0:13:18.\n",
            "  Batch 1,880  of  3,375.    Elapsed: 0:13:36.\n",
            "  Batch 1,920  of  3,375.    Elapsed: 0:13:53.\n",
            "  Batch 1,960  of  3,375.    Elapsed: 0:14:10.\n",
            "  Batch 2,000  of  3,375.    Elapsed: 0:14:28.\n",
            "  Batch 2,040  of  3,375.    Elapsed: 0:14:45.\n",
            "  Batch 2,080  of  3,375.    Elapsed: 0:15:03.\n",
            "  Batch 2,120  of  3,375.    Elapsed: 0:15:20.\n",
            "  Batch 2,160  of  3,375.    Elapsed: 0:15:38.\n",
            "  Batch 2,200  of  3,375.    Elapsed: 0:15:55.\n",
            "  Batch 2,240  of  3,375.    Elapsed: 0:16:13.\n",
            "  Batch 2,280  of  3,375.    Elapsed: 0:16:30.\n",
            "  Batch 2,320  of  3,375.    Elapsed: 0:16:48.\n",
            "  Batch 2,360  of  3,375.    Elapsed: 0:17:05.\n",
            "  Batch 2,400  of  3,375.    Elapsed: 0:17:23.\n",
            "  Batch 2,440  of  3,375.    Elapsed: 0:17:40.\n",
            "  Batch 2,480  of  3,375.    Elapsed: 0:17:57.\n",
            "  Batch 2,520  of  3,375.    Elapsed: 0:18:15.\n",
            "  Batch 2,560  of  3,375.    Elapsed: 0:18:32.\n",
            "  Batch 2,600  of  3,375.    Elapsed: 0:18:50.\n",
            "  Batch 2,640  of  3,375.    Elapsed: 0:19:07.\n",
            "  Batch 2,680  of  3,375.    Elapsed: 0:19:25.\n",
            "  Batch 2,720  of  3,375.    Elapsed: 0:19:42.\n",
            "  Batch 2,760  of  3,375.    Elapsed: 0:20:00.\n",
            "  Batch 2,800  of  3,375.    Elapsed: 0:20:17.\n",
            "  Batch 2,840  of  3,375.    Elapsed: 0:20:35.\n",
            "  Batch 2,880  of  3,375.    Elapsed: 0:20:52.\n",
            "  Batch 2,920  of  3,375.    Elapsed: 0:21:10.\n",
            "  Batch 2,960  of  3,375.    Elapsed: 0:21:27.\n",
            "  Batch 3,000  of  3,375.    Elapsed: 0:21:45.\n",
            "  Batch 3,040  of  3,375.    Elapsed: 0:22:02.\n",
            "  Batch 3,080  of  3,375.    Elapsed: 0:22:20.\n",
            "  Batch 3,120  of  3,375.    Elapsed: 0:22:37.\n",
            "  Batch 3,160  of  3,375.    Elapsed: 0:22:55.\n",
            "  Batch 3,200  of  3,375.    Elapsed: 0:23:12.\n",
            "  Batch 3,240  of  3,375.    Elapsed: 0:23:30.\n",
            "  Batch 3,280  of  3,375.    Elapsed: 0:23:47.\n",
            "  Batch 3,320  of  3,375.    Elapsed: 0:24:05.\n",
            "  Batch 3,360  of  3,375.    Elapsed: 0:24:22.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epcoh took: 0:24:29\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:47\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  3,375.    Elapsed: 0:00:17.\n",
            "  Batch    80  of  3,375.    Elapsed: 0:00:35.\n",
            "  Batch   120  of  3,375.    Elapsed: 0:00:52.\n",
            "  Batch   160  of  3,375.    Elapsed: 0:01:10.\n",
            "  Batch   200  of  3,375.    Elapsed: 0:01:27.\n",
            "  Batch   240  of  3,375.    Elapsed: 0:01:45.\n",
            "  Batch   280  of  3,375.    Elapsed: 0:02:02.\n",
            "  Batch   320  of  3,375.    Elapsed: 0:02:20.\n",
            "  Batch   360  of  3,375.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  3,375.    Elapsed: 0:02:55.\n",
            "  Batch   440  of  3,375.    Elapsed: 0:03:12.\n",
            "  Batch   480  of  3,375.    Elapsed: 0:03:29.\n",
            "  Batch   520  of  3,375.    Elapsed: 0:03:47.\n",
            "  Batch   560  of  3,375.    Elapsed: 0:04:04.\n",
            "  Batch   600  of  3,375.    Elapsed: 0:04:22.\n",
            "  Batch   640  of  3,375.    Elapsed: 0:04:39.\n",
            "  Batch   680  of  3,375.    Elapsed: 0:04:57.\n",
            "  Batch   720  of  3,375.    Elapsed: 0:05:14.\n",
            "  Batch   760  of  3,375.    Elapsed: 0:05:32.\n",
            "  Batch   800  of  3,375.    Elapsed: 0:05:49.\n",
            "  Batch   840  of  3,375.    Elapsed: 0:06:07.\n",
            "  Batch   880  of  3,375.    Elapsed: 0:06:24.\n",
            "  Batch   920  of  3,375.    Elapsed: 0:06:42.\n",
            "  Batch   960  of  3,375.    Elapsed: 0:06:59.\n",
            "  Batch 1,000  of  3,375.    Elapsed: 0:07:17.\n",
            "  Batch 1,040  of  3,375.    Elapsed: 0:07:34.\n",
            "  Batch 1,080  of  3,375.    Elapsed: 0:07:52.\n",
            "  Batch 1,120  of  3,375.    Elapsed: 0:08:09.\n",
            "  Batch 1,160  of  3,375.    Elapsed: 0:08:27.\n",
            "  Batch 1,200  of  3,375.    Elapsed: 0:08:44.\n",
            "  Batch 1,240  of  3,375.    Elapsed: 0:09:02.\n",
            "  Batch 1,280  of  3,375.    Elapsed: 0:09:19.\n",
            "  Batch 1,320  of  3,375.    Elapsed: 0:09:37.\n",
            "  Batch 1,360  of  3,375.    Elapsed: 0:09:54.\n",
            "  Batch 1,400  of  3,375.    Elapsed: 0:10:12.\n",
            "  Batch 1,440  of  3,375.    Elapsed: 0:10:29.\n",
            "  Batch 1,480  of  3,375.    Elapsed: 0:10:47.\n",
            "  Batch 1,520  of  3,375.    Elapsed: 0:11:04.\n",
            "  Batch 1,560  of  3,375.    Elapsed: 0:11:22.\n",
            "  Batch 1,600  of  3,375.    Elapsed: 0:11:39.\n",
            "  Batch 1,640  of  3,375.    Elapsed: 0:11:56.\n",
            "  Batch 1,680  of  3,375.    Elapsed: 0:12:14.\n",
            "  Batch 1,720  of  3,375.    Elapsed: 0:12:31.\n",
            "  Batch 1,760  of  3,375.    Elapsed: 0:12:49.\n",
            "  Batch 1,800  of  3,375.    Elapsed: 0:13:06.\n",
            "  Batch 1,840  of  3,375.    Elapsed: 0:13:24.\n",
            "  Batch 1,880  of  3,375.    Elapsed: 0:13:41.\n",
            "  Batch 1,920  of  3,375.    Elapsed: 0:13:59.\n",
            "  Batch 1,960  of  3,375.    Elapsed: 0:14:16.\n",
            "  Batch 2,000  of  3,375.    Elapsed: 0:14:34.\n",
            "  Batch 2,040  of  3,375.    Elapsed: 0:14:51.\n",
            "  Batch 2,080  of  3,375.    Elapsed: 0:15:09.\n",
            "  Batch 2,120  of  3,375.    Elapsed: 0:15:26.\n",
            "  Batch 2,160  of  3,375.    Elapsed: 0:15:44.\n",
            "  Batch 2,200  of  3,375.    Elapsed: 0:16:01.\n",
            "  Batch 2,240  of  3,375.    Elapsed: 0:16:19.\n",
            "  Batch 2,280  of  3,375.    Elapsed: 0:16:36.\n",
            "  Batch 2,320  of  3,375.    Elapsed: 0:16:54.\n",
            "  Batch 2,360  of  3,375.    Elapsed: 0:17:11.\n",
            "  Batch 2,400  of  3,375.    Elapsed: 0:17:29.\n",
            "  Batch 2,440  of  3,375.    Elapsed: 0:17:46.\n",
            "  Batch 2,480  of  3,375.    Elapsed: 0:18:04.\n",
            "  Batch 2,520  of  3,375.    Elapsed: 0:18:21.\n",
            "  Batch 2,560  of  3,375.    Elapsed: 0:18:39.\n",
            "  Batch 2,600  of  3,375.    Elapsed: 0:18:56.\n",
            "  Batch 2,640  of  3,375.    Elapsed: 0:19:14.\n",
            "  Batch 2,680  of  3,375.    Elapsed: 0:19:31.\n",
            "  Batch 2,720  of  3,375.    Elapsed: 0:19:49.\n",
            "  Batch 2,760  of  3,375.    Elapsed: 0:20:06.\n",
            "  Batch 2,800  of  3,375.    Elapsed: 0:20:24.\n",
            "  Batch 2,840  of  3,375.    Elapsed: 0:20:41.\n",
            "  Batch 2,880  of  3,375.    Elapsed: 0:20:59.\n",
            "  Batch 2,920  of  3,375.    Elapsed: 0:21:16.\n",
            "  Batch 2,960  of  3,375.    Elapsed: 0:21:34.\n",
            "  Batch 3,000  of  3,375.    Elapsed: 0:21:51.\n",
            "  Batch 3,040  of  3,375.    Elapsed: 0:22:09.\n",
            "  Batch 3,080  of  3,375.    Elapsed: 0:22:26.\n",
            "  Batch 3,120  of  3,375.    Elapsed: 0:22:44.\n",
            "  Batch 3,160  of  3,375.    Elapsed: 0:23:01.\n",
            "  Batch 3,200  of  3,375.    Elapsed: 0:23:19.\n",
            "  Batch 3,240  of  3,375.    Elapsed: 0:23:36.\n",
            "  Batch 3,280  of  3,375.    Elapsed: 0:23:54.\n",
            "  Batch 3,320  of  3,375.    Elapsed: 0:24:11.\n",
            "  Batch 3,360  of  3,375.    Elapsed: 0:24:28.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:24:35\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation took: 0:00:47\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  3,375.    Elapsed: 0:00:17.\n",
            "  Batch    80  of  3,375.    Elapsed: 0:00:35.\n",
            "  Batch   120  of  3,375.    Elapsed: 0:00:52.\n",
            "  Batch   160  of  3,375.    Elapsed: 0:01:10.\n",
            "  Batch   200  of  3,375.    Elapsed: 0:01:27.\n",
            "  Batch   240  of  3,375.    Elapsed: 0:01:45.\n",
            "  Batch   280  of  3,375.    Elapsed: 0:02:02.\n",
            "  Batch   320  of  3,375.    Elapsed: 0:02:20.\n",
            "  Batch   360  of  3,375.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  3,375.    Elapsed: 0:02:55.\n",
            "  Batch   440  of  3,375.    Elapsed: 0:03:12.\n",
            "  Batch   480  of  3,375.    Elapsed: 0:03:29.\n",
            "  Batch   520  of  3,375.    Elapsed: 0:03:47.\n",
            "  Batch   560  of  3,375.    Elapsed: 0:04:04.\n",
            "  Batch   600  of  3,375.    Elapsed: 0:04:22.\n",
            "  Batch   640  of  3,375.    Elapsed: 0:04:39.\n",
            "  Batch   680  of  3,375.    Elapsed: 0:04:57.\n",
            "  Batch   720  of  3,375.    Elapsed: 0:05:14.\n",
            "  Batch   760  of  3,375.    Elapsed: 0:05:32.\n",
            "  Batch   800  of  3,375.    Elapsed: 0:05:49.\n",
            "  Batch   840  of  3,375.    Elapsed: 0:06:07.\n",
            "  Batch   880  of  3,375.    Elapsed: 0:06:24.\n",
            "  Batch   920  of  3,375.    Elapsed: 0:06:42.\n",
            "  Batch   960  of  3,375.    Elapsed: 0:06:59.\n",
            "  Batch 1,000  of  3,375.    Elapsed: 0:07:17.\n",
            "  Batch 1,040  of  3,375.    Elapsed: 0:07:34.\n",
            "  Batch 1,080  of  3,375.    Elapsed: 0:07:51.\n",
            "  Batch 1,120  of  3,375.    Elapsed: 0:08:09.\n",
            "  Batch 1,160  of  3,375.    Elapsed: 0:08:26.\n",
            "  Batch 1,200  of  3,375.    Elapsed: 0:08:44.\n",
            "  Batch 1,240  of  3,375.    Elapsed: 0:09:01.\n",
            "  Batch 1,280  of  3,375.    Elapsed: 0:09:19.\n",
            "  Batch 1,320  of  3,375.    Elapsed: 0:09:36.\n",
            "  Batch 1,360  of  3,375.    Elapsed: 0:09:54.\n",
            "  Batch 1,400  of  3,375.    Elapsed: 0:10:11.\n",
            "  Batch 1,440  of  3,375.    Elapsed: 0:10:29.\n",
            "  Batch 1,480  of  3,375.    Elapsed: 0:10:46.\n",
            "  Batch 1,520  of  3,375.    Elapsed: 0:11:04.\n",
            "  Batch 1,560  of  3,375.    Elapsed: 0:11:21.\n",
            "  Batch 1,600  of  3,375.    Elapsed: 0:11:39.\n",
            "  Batch 1,640  of  3,375.    Elapsed: 0:11:56.\n",
            "  Batch 1,680  of  3,375.    Elapsed: 0:12:14.\n",
            "  Batch 1,720  of  3,375.    Elapsed: 0:12:31.\n",
            "  Batch 1,760  of  3,375.    Elapsed: 0:12:49.\n",
            "  Batch 1,800  of  3,375.    Elapsed: 0:13:06.\n",
            "  Batch 1,840  of  3,375.    Elapsed: 0:13:24.\n",
            "  Batch 1,880  of  3,375.    Elapsed: 0:13:41.\n",
            "  Batch 1,920  of  3,375.    Elapsed: 0:13:59.\n",
            "  Batch 1,960  of  3,375.    Elapsed: 0:14:16.\n",
            "  Batch 2,000  of  3,375.    Elapsed: 0:14:34.\n",
            "  Batch 2,040  of  3,375.    Elapsed: 0:14:51.\n",
            "  Batch 2,080  of  3,375.    Elapsed: 0:15:09.\n",
            "  Batch 2,120  of  3,375.    Elapsed: 0:15:26.\n",
            "  Batch 2,160  of  3,375.    Elapsed: 0:15:43.\n",
            "  Batch 2,200  of  3,375.    Elapsed: 0:16:01.\n",
            "  Batch 2,240  of  3,375.    Elapsed: 0:16:18.\n",
            "  Batch 2,280  of  3,375.    Elapsed: 0:16:36.\n",
            "  Batch 2,320  of  3,375.    Elapsed: 0:16:53.\n",
            "  Batch 2,360  of  3,375.    Elapsed: 0:17:11.\n",
            "  Batch 2,400  of  3,375.    Elapsed: 0:17:28.\n",
            "  Batch 2,440  of  3,375.    Elapsed: 0:17:46.\n",
            "  Batch 2,480  of  3,375.    Elapsed: 0:18:03.\n",
            "  Batch 2,520  of  3,375.    Elapsed: 0:18:21.\n",
            "  Batch 2,560  of  3,375.    Elapsed: 0:18:38.\n",
            "  Batch 2,600  of  3,375.    Elapsed: 0:18:56.\n",
            "  Batch 2,640  of  3,375.    Elapsed: 0:19:13.\n",
            "  Batch 2,680  of  3,375.    Elapsed: 0:19:31.\n",
            "  Batch 2,720  of  3,375.    Elapsed: 0:19:48.\n",
            "  Batch 2,760  of  3,375.    Elapsed: 0:20:06.\n",
            "  Batch 2,800  of  3,375.    Elapsed: 0:20:23.\n",
            "  Batch 2,840  of  3,375.    Elapsed: 0:20:41.\n",
            "  Batch 2,880  of  3,375.    Elapsed: 0:20:58.\n",
            "  Batch 2,920  of  3,375.    Elapsed: 0:21:16.\n",
            "  Batch 2,960  of  3,375.    Elapsed: 0:21:33.\n",
            "  Batch 3,000  of  3,375.    Elapsed: 0:21:51.\n",
            "  Batch 3,040  of  3,375.    Elapsed: 0:22:08.\n",
            "  Batch 3,080  of  3,375.    Elapsed: 0:22:26.\n",
            "  Batch 3,120  of  3,375.    Elapsed: 0:22:43.\n",
            "  Batch 3,160  of  3,375.    Elapsed: 0:23:01.\n",
            "  Batch 3,200  of  3,375.    Elapsed: 0:23:18.\n",
            "  Batch 3,240  of  3,375.    Elapsed: 0:23:36.\n",
            "  Batch 3,280  of  3,375.    Elapsed: 0:23:53.\n",
            "  Batch 3,320  of  3,375.    Elapsed: 0:24:11.\n",
            "  Batch 3,360  of  3,375.    Elapsed: 0:24:28.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:24:35\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation took: 0:00:47\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  3,375.    Elapsed: 0:00:17.\n",
            "  Batch    80  of  3,375.    Elapsed: 0:00:35.\n",
            "  Batch   120  of  3,375.    Elapsed: 0:00:52.\n",
            "  Batch   160  of  3,375.    Elapsed: 0:01:10.\n",
            "  Batch   200  of  3,375.    Elapsed: 0:01:27.\n",
            "  Batch   240  of  3,375.    Elapsed: 0:01:45.\n",
            "  Batch   280  of  3,375.    Elapsed: 0:02:02.\n",
            "  Batch   320  of  3,375.    Elapsed: 0:02:20.\n",
            "  Batch   360  of  3,375.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  3,375.    Elapsed: 0:02:55.\n",
            "  Batch   440  of  3,375.    Elapsed: 0:03:12.\n",
            "  Batch   480  of  3,375.    Elapsed: 0:03:29.\n",
            "  Batch   520  of  3,375.    Elapsed: 0:03:47.\n",
            "  Batch   560  of  3,375.    Elapsed: 0:04:05.\n",
            "  Batch   600  of  3,375.    Elapsed: 0:04:22.\n",
            "  Batch   640  of  3,375.    Elapsed: 0:04:39.\n",
            "  Batch   680  of  3,375.    Elapsed: 0:04:57.\n",
            "  Batch   720  of  3,375.    Elapsed: 0:05:14.\n",
            "  Batch   760  of  3,375.    Elapsed: 0:05:32.\n",
            "  Batch   800  of  3,375.    Elapsed: 0:05:49.\n",
            "  Batch   840  of  3,375.    Elapsed: 0:06:07.\n",
            "  Batch   880  of  3,375.    Elapsed: 0:06:24.\n",
            "  Batch   920  of  3,375.    Elapsed: 0:06:42.\n",
            "  Batch   960  of  3,375.    Elapsed: 0:06:59.\n",
            "  Batch 1,000  of  3,375.    Elapsed: 0:07:17.\n",
            "  Batch 1,040  of  3,375.    Elapsed: 0:07:34.\n",
            "  Batch 1,080  of  3,375.    Elapsed: 0:07:52.\n",
            "  Batch 1,120  of  3,375.    Elapsed: 0:08:09.\n",
            "  Batch 1,160  of  3,375.    Elapsed: 0:08:27.\n",
            "  Batch 1,200  of  3,375.    Elapsed: 0:08:44.\n",
            "  Batch 1,240  of  3,375.    Elapsed: 0:09:01.\n",
            "  Batch 1,280  of  3,375.    Elapsed: 0:09:19.\n",
            "  Batch 1,320  of  3,375.    Elapsed: 0:09:36.\n",
            "  Batch 1,360  of  3,375.    Elapsed: 0:09:54.\n",
            "  Batch 1,400  of  3,375.    Elapsed: 0:10:11.\n",
            "  Batch 1,440  of  3,375.    Elapsed: 0:10:29.\n",
            "  Batch 1,480  of  3,375.    Elapsed: 0:10:46.\n",
            "  Batch 1,520  of  3,375.    Elapsed: 0:11:04.\n",
            "  Batch 1,560  of  3,375.    Elapsed: 0:11:21.\n",
            "  Batch 1,600  of  3,375.    Elapsed: 0:11:39.\n",
            "  Batch 1,640  of  3,375.    Elapsed: 0:11:56.\n",
            "  Batch 1,680  of  3,375.    Elapsed: 0:12:14.\n",
            "  Batch 1,720  of  3,375.    Elapsed: 0:12:31.\n",
            "  Batch 1,760  of  3,375.    Elapsed: 0:12:49.\n",
            "  Batch 1,800  of  3,375.    Elapsed: 0:13:06.\n",
            "  Batch 1,840  of  3,375.    Elapsed: 0:13:23.\n",
            "  Batch 1,880  of  3,375.    Elapsed: 0:13:41.\n",
            "  Batch 1,920  of  3,375.    Elapsed: 0:13:58.\n",
            "  Batch 1,960  of  3,375.    Elapsed: 0:14:16.\n",
            "  Batch 2,000  of  3,375.    Elapsed: 0:14:33.\n",
            "  Batch 2,040  of  3,375.    Elapsed: 0:14:51.\n",
            "  Batch 2,080  of  3,375.    Elapsed: 0:15:08.\n",
            "  Batch 2,120  of  3,375.    Elapsed: 0:15:26.\n",
            "  Batch 2,160  of  3,375.    Elapsed: 0:15:43.\n",
            "  Batch 2,200  of  3,375.    Elapsed: 0:16:01.\n",
            "  Batch 2,240  of  3,375.    Elapsed: 0:16:18.\n",
            "  Batch 2,280  of  3,375.    Elapsed: 0:16:36.\n",
            "  Batch 2,320  of  3,375.    Elapsed: 0:16:53.\n",
            "  Batch 2,360  of  3,375.    Elapsed: 0:17:11.\n",
            "  Batch 2,400  of  3,375.    Elapsed: 0:17:28.\n",
            "  Batch 2,440  of  3,375.    Elapsed: 0:17:46.\n",
            "  Batch 2,480  of  3,375.    Elapsed: 0:18:03.\n",
            "  Batch 2,520  of  3,375.    Elapsed: 0:18:21.\n",
            "  Batch 2,560  of  3,375.    Elapsed: 0:18:38.\n",
            "  Batch 2,600  of  3,375.    Elapsed: 0:18:56.\n",
            "  Batch 2,640  of  3,375.    Elapsed: 0:19:13.\n",
            "  Batch 2,680  of  3,375.    Elapsed: 0:19:31.\n",
            "  Batch 2,720  of  3,375.    Elapsed: 0:19:48.\n",
            "  Batch 2,760  of  3,375.    Elapsed: 0:20:05.\n",
            "  Batch 2,800  of  3,375.    Elapsed: 0:20:23.\n",
            "  Batch 2,840  of  3,375.    Elapsed: 0:20:40.\n",
            "  Batch 2,880  of  3,375.    Elapsed: 0:20:58.\n",
            "  Batch 2,920  of  3,375.    Elapsed: 0:21:15.\n",
            "  Batch 2,960  of  3,375.    Elapsed: 0:21:33.\n",
            "  Batch 3,000  of  3,375.    Elapsed: 0:21:50.\n",
            "  Batch 3,040  of  3,375.    Elapsed: 0:22:08.\n",
            "  Batch 3,080  of  3,375.    Elapsed: 0:22:25.\n",
            "  Batch 3,120  of  3,375.    Elapsed: 0:22:43.\n",
            "  Batch 3,160  of  3,375.    Elapsed: 0:23:00.\n",
            "  Batch 3,200  of  3,375.    Elapsed: 0:23:18.\n",
            "  Batch 3,240  of  3,375.    Elapsed: 0:23:35.\n",
            "  Batch 3,280  of  3,375.    Elapsed: 0:23:53.\n",
            "  Batch 3,320  of  3,375.    Elapsed: 0:24:10.\n",
            "  Batch 3,360  of  3,375.    Elapsed: 0:24:28.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:24:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAkvLl9pAYoo",
        "colab_type": "text"
      },
      "source": [
        "Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmcj6mPBAYop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "17a43dc1-1730-49f8-f6b6-52c2ac5eac88"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_values, 'b-o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vyWFO7rAYor",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPBuLiXxAYos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e7ea777d-05ca-4add-f399-a1ed96585c05"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(\"./IMDB_Dataset\", delimiter=',', header=None, names=['review', 'sentiment'])\n",
        "#sentences = df.review.values[1:1000]\n",
        "#labels = df.sentiment.values[1:1000]\n",
        "#labels = [1 if l == 'positive' else 0 for l in labels]\n",
        "\n",
        "df = pd.read_csv(\"./Dataset/ag_news_test.csv\", delimiter=',', header=None, names=['category', \"head\", 'content'])\n",
        "\n",
        "# Print number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "sentences = df.content.values\n",
        "labels = df.category.values - 1\n",
        "\n",
        "input_ids = []\n",
        "\n",
        "for s in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "        s,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    input_ids.append(encoded_sent)\n",
        "        \n",
        "add_padding_and_truncate(input_ids)\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i > 0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "    \n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "#predictions = []\n",
        "#true_labels = []\n",
        "\n",
        "eval_accuracy = 0\n",
        "eval_steps = 0\n",
        "for batch in prediction_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    #predictions.append(logits)\n",
        "    #true_labels.append(label_ids)\n",
        "    \n",
        "    eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    eval_steps += 1\n",
        "\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/eval_steps))\n",
        "print(\"Done\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 7,600\n",
            "\n",
            "Accuracy: 0.94\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}