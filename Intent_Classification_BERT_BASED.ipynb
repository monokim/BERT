{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/monokim/BERT/blob/master/Intent_Classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdw6bkfeAYn0"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l3MqPxeAYn3"
   },
   "source": [
    "Check If there is a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n1U_spRMAYn4",
    "outputId": "cfabfbf4-f3d4-45bb-8a18-26ce6be37dcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GeForce GTX 1060 will be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0), 'will be used.')\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'xlnet' # bert, albert, roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxVfEMVkAYn8"
   },
   "source": [
    "IMDB Movie review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "TS9mqU28FZU8",
    "outputId": "5f55c4b5-cdd4-4747-9b7c-1e995fdf31df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 50,001\n",
      "\n",
      "[\"This is like a zoology textbook, given that its depiction of animals is so accurate. However, here are a few details that appear to have been slightly modified during the transition to film:<br /><br />- Handgun bullets never hit giant Komodo dragons. It doesn't matter how many times you shoot at the Komodo, bullets just won't go near it.<br /><br />- The best way to avoid being eaten by a giant Cobra, or a giant Komodo dragon, is just to stand there. The exception to this rule is if you've been told to stay very still, in which case you should run off, until the Komodo is right next to you, and then you should stand there, expecting defeat.<br /><br />- Minutes of choppy slow motion footage behind the credits really makes for enjoyable watching.<br /><br />- $5,000 is a memory enhancement tool, and an ample substitute for losing your boating license/getting arrested.<br /><br />- Members of elite army units don't see giant Komodo dragons coming until they are within one metre of the over-sized beings. Maybe the computer-generated nature of these dragons has something to do with it.<br /><br />- When filming a news story aiming on exposing illegal animal testing, a reporter and a cameraman with one camera is all the gear and personnel you will need; sound gear, a second camera, microphones etc are all superfluous.<br /><br />- When you hear a loud animal scream, and one person has a gun, he should take it out and point it at the nearest person.<br /><br />- When you take a gun out, the sound of the safety being taken off will be made, even if your finger is nowhere near the safety<br /><br />- Reporters agree to go half-way around the world in order to expose something - without having the faintest idea what they're exposing. Background research and vague knowledge are out of fashion in modern journalism.<br /><br />- Handguns hold at least 52 bullets in one clip, and then more than that in the next clip. Despite that, those with guns claim that they will need more ammo.<br /><br />- Expensive cameras (also, remember that the reporter only has one camera) are regularly left behind without even a moment's hesitation or regret. These cameras amazingly manage to make their way back to the reporter all by themselves.<br /><br />- The blonde girl really is the stupid one.<br /><br />- The same girl that says not to go into a house because a Komodo dragon can easily run right through it, thus making it unsafe, takes a team into a building made of the same material for protection - and nobody says a word about it.<br /><br />- High-tech facilities look like simple offices with high school chemistry sets.<br /><br />- Genetically-modified snakes grow from normal size to 100 feet long in a matter of a day, but don't grow at all in the weeks either side.<br /><br />- The military routinely destroys entire islands when people don't meet contact deadlines.<br /><br />- Men with guns don't necessarily change the direction they're shooting when their target is no longer right in front of them. Instead, they just keep shooting into the air.<br /><br />- The better looking you are, the greater your chance of surviving giant creatures.<br /><br />- Women's intuition is reliable enough to change even the most stubborn of minds.<br /><br />- Any time you're being hunted by giant creatures is a great time to hit on girls half your age.<br /><br />- Animal noises are an appropriate masking noise for 'swearing' at the same volume.<br /><br />- Old Israeli and Russian planes are regularly used by the US Military. [SEP] [CLS]\", \"This movie is awful, I can't even be bothered to write a review on this garbage! All i will say it is one of the most boring films I've ever seen.<br /><br />And the acting is very bad. The boy who plays the main character really annoys me, he's got the same expression on his face through out the movie. I just want to slap him! Basically 80% of the movie is slow motion shots of skateboarders, weird music, and utter sh*t..<br /><br />Apparently I've got to write at least 10 lines of text to submit this comment, so I'll use up a few more lines by saying the lead character has got one of those faces you just want to slap!<br /><br />Meh i give up..THIS MOVIE SUCKS !!!! [SEP] [CLS]\", \"Why do movie makers always go against the author's work? I mean, yes, things have to be condensed for the sake of viewer interest, but look at Anne of Green Gables. They did a wonderful job of combining important events into a cohesive whole that was simply delightful. I can't believe that they chose to combine three novels together for Anne of Avonlea into such a dreadful mess. Look at all they missed out on by doing that . . . Paul Irving, little Elizabeth, the widows, Windy Poplars . . . and Anne's college years, for heaven's sake!!! Wouldn't it have been delightful to meet Priscilla and all the rest of the Redmond gang? Kevin Sullivan should have taken things one movie at a time, instead of jumbling them all together and combining characters and events the way he did. This movie was good, if you leave the novels out of it!! But L.M. Montgomery's beautiful work is something that should not be denied. This movie was a let down after seeing the successful way he brough Anne of Green Gables to life. [SEP] [CLS]\"]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data set into a pandas dataframe\n",
    "df = pd.read_csv(\"./Dataset/IMDB_Dataset.csv\", delimiter=',', header=None, names=['review', 'sentiment'])\n",
    "\n",
    "# Print number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "labels = df.sentiment.values[1000:5000]\n",
    "labels = [1 if l == 'positive' else 0 for l in labels]\n",
    "sentences = df.review.values[1000:5000]\n",
    "if bert_model == 'xlnet':\n",
    "    sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "\n",
    "print(sentences[:3])\n",
    "print(labels[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3v5awyN2bNTG"
   },
   "source": [
    "AG NEWS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "jU3JutbJbNnU",
    "outputId": "a5a5c243-a322-4ac0-c10c-fce8a03fc674"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\n# Load the data set into a pandas dataframe\\ndf = pd.read_csv(\"./Dataset/ag_news_train.csv\", delimiter=\\',\\', header=None, names=[\\'category\\', \"head\", \\'content\\'])\\n\\n# Print number of sentences.\\nprint(\\'Number of training sentences: {:,}\\n\\'.format(df.shape[0]))\\nlabels = df.category.values - 1\\nsentences = df.content.values\\nif bert_model == \\'xlnet\\':\\n    sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\\n\\nprint(sentences[:3])\\nprint(labels[:3])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data set into a pandas dataframe\n",
    "df = pd.read_csv(\"./Dataset/ag_news_train.csv\", delimiter=',', header=None, names=['category', \"head\", 'content'])\n",
    "\n",
    "# Print number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "labels = df.category.values - 1\n",
    "sentences = df.content.values\n",
    "if bert_model == 'xlnet':\n",
    "    sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "\n",
    "print(sentences[:3])\n",
    "print(labels[:3])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJ2Sv3TkAYoG"
   },
   "source": [
    "# BERT Tokenizer\n",
    "\n",
    "Tokenize each words and convert to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "uiMqEtkd3T7o",
    "outputId": "41a14a97-2ec1-46be-e7e4-8fa1e27529c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mingo\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (1.12.14)\n",
      "Requirement already satisfied: requests in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (4.31.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (1.16.2)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.14 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.15.14)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.14->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mingo\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.14->boto3->transformers) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Install transformers by using pip\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "referenced_widgets": [
      "f05b76c973f249519c510b7aade21510",
      "9250a549d7fd40ad93d2cdd12fb6f91f",
      "ad81993d700646b497f8fbcf2423fe15",
      "cb136730dc874ddb88ff884956c373c8",
      "af99d4addbca4e56970fb2a84d59ed74",
      "a4aa805f33c044f58c4d0dc5611364e1",
      "29ab35ebeb04449c97bfff988caaeb57",
      "f209950dc361472b88dca6a52006ea59"
     ]
    },
    "colab_type": "code",
    "id": "GrqOOw-LAYoH",
    "outputId": "0359a748-9e24-41e4-ac00-f3aa19c373c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :  This is like a zoology textbook, given that its depiction of animals is so accurate. However, here are a few details that appear to have been slightly modified during the transition to film:<br /><br />- Handgun bullets never hit giant Komodo dragons. It doesn't matter how many times you shoot at the Komodo, bullets just won't go near it.<br /><br />- The best way to avoid being eaten by a giant Cobra, or a giant Komodo dragon, is just to stand there. The exception to this rule is if you've been told to stay very still, in which case you should run off, until the Komodo is right next to you, and then you should stand there, expecting defeat.<br /><br />- Minutes of choppy slow motion footage behind the credits really makes for enjoyable watching.<br /><br />- $5,000 is a memory enhancement tool, and an ample substitute for losing your boating license/getting arrested.<br /><br />- Members of elite army units don't see giant Komodo dragons coming until they are within one metre of the over-sized beings. Maybe the computer-generated nature of these dragons has something to do with it.<br /><br />- When filming a news story aiming on exposing illegal animal testing, a reporter and a cameraman with one camera is all the gear and personnel you will need; sound gear, a second camera, microphones etc are all superfluous.<br /><br />- When you hear a loud animal scream, and one person has a gun, he should take it out and point it at the nearest person.<br /><br />- When you take a gun out, the sound of the safety being taken off will be made, even if your finger is nowhere near the safety<br /><br />- Reporters agree to go half-way around the world in order to expose something - without having the faintest idea what they're exposing. Background research and vague knowledge are out of fashion in modern journalism.<br /><br />- Handguns hold at least 52 bullets in one clip, and then more than that in the next clip. Despite that, those with guns claim that they will need more ammo.<br /><br />- Expensive cameras (also, remember that the reporter only has one camera) are regularly left behind without even a moment's hesitation or regret. These cameras amazingly manage to make their way back to the reporter all by themselves.<br /><br />- The blonde girl really is the stupid one.<br /><br />- The same girl that says not to go into a house because a Komodo dragon can easily run right through it, thus making it unsafe, takes a team into a building made of the same material for protection - and nobody says a word about it.<br /><br />- High-tech facilities look like simple offices with high school chemistry sets.<br /><br />- Genetically-modified snakes grow from normal size to 100 feet long in a matter of a day, but don't grow at all in the weeks either side.<br /><br />- The military routinely destroys entire islands when people don't meet contact deadlines.<br /><br />- Men with guns don't necessarily change the direction they're shooting when their target is no longer right in front of them. Instead, they just keep shooting into the air.<br /><br />- The better looking you are, the greater your chance of surviving giant creatures.<br /><br />- Women's intuition is reliable enough to change even the most stubborn of minds.<br /><br />- Any time you're being hunted by giant creatures is a great time to hit on girls half your age.<br /><br />- Animal noises are an appropriate masking noise for 'swearing' at the same volume.<br /><br />- Old Israeli and Russian planes are regularly used by the US Military. [SEP] [CLS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized :  ['▁this', '▁is', '▁like', '▁a', '▁', 'zo', 'ology', '▁textbook', ',', '▁given', '▁that', '▁its', '▁depiction', '▁of', '▁animals', '▁is', '▁so', '▁accurate', '.', '▁however', ',', '▁here', '▁are', '▁a', '▁few', '▁details', '▁that', '▁appear', '▁to', '▁have', '▁been', '▁slightly', '▁modified', '▁during', '▁the', '▁transition', '▁to', '▁film', ':', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁handgun', '▁bullets', '▁never', '▁hit', '▁giant', '▁', 'kom', 'odo', '▁dragon', 's', '.', '▁it', '▁doesn', \"'\", 't', '▁matter', '▁how', '▁many', '▁times', '▁you', '▁shoot', '▁at', '▁the', '▁', 'kom', 'odo', ',', '▁bullets', '▁just', '▁won', \"'\", 't', '▁go', '▁near', '▁it', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁the', '▁best', '▁way', '▁to', '▁avoid', '▁being', '▁eaten', '▁by', '▁a', '▁giant', '▁co', 'bra', ',', '▁or', '▁a', '▁giant', '▁', 'kom', 'odo', '▁dragon', ',', '▁is', '▁just', '▁to', '▁stand', '▁there', '.', '▁the', '▁exception', '▁to', '▁this', '▁rule', '▁is', '▁if', '▁you', \"'\", 've', '▁been', '▁told', '▁to', '▁stay', '▁very', '▁still', ',', '▁in', '▁which', '▁case', '▁you', '▁should', '▁run', '▁off', ',', '▁until', '▁the', '▁', 'kom', 'odo', '▁is', '▁right', '▁next', '▁to', '▁you', ',', '▁and', '▁then', '▁you', '▁should', '▁stand', '▁there', ',', '▁expecting', '▁defeat', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁minutes', '▁of', '▁chop', 'py', '▁slow', '▁motion', '▁footage', '▁behind', '▁the', '▁credits', '▁really', '▁makes', '▁for', '▁enjoyable', '▁watching', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁$5,000', '▁is', '▁a', '▁memory', '▁enhancement', '▁tool', ',', '▁and', '▁an', '▁ample', '▁substitute', '▁for', '▁losing', '▁your', '▁boat', 'ing', '▁license', '/', 'getting', '▁arrested', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁members', '▁of', '▁elite', '▁army', '▁units', '▁don', \"'\", 't', '▁see', '▁giant', '▁', 'kom', 'odo', '▁dragon', 's', '▁coming', '▁until', '▁they', '▁are', '▁within', '▁one', '▁', 'metre', '▁of', '▁the', '▁over', '-', 'sized', '▁being', 's', '.', '▁maybe', '▁the', '▁computer', '-', 'generated', '▁nature', '▁of', '▁these', '▁dragon', 's', '▁has', '▁something', '▁to', '▁do', '▁with', '▁it', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁when', '▁filming', '▁a', '▁news', '▁story', '▁aiming', '▁on', '▁exposing', '▁illegal', '▁animal', '▁testing', ',', '▁a', '▁reporter', '▁and', '▁a', '▁camera', 'man', '▁with', '▁one', '▁camera', '▁is', '▁all', '▁the', '▁gear', '▁and', '▁personnel', '▁you', '▁will', '▁need', ';', '▁sound', '▁gear', ',', '▁a', '▁second', '▁camera', ',', '▁microphone', 's', '▁etc', '▁are', '▁all', '▁super', 'flu', 'ous', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁when', '▁you', '▁hear', '▁a', '▁loud', '▁animal', '▁scream', ',', '▁and', '▁one', '▁person', '▁has', '▁a', '▁gun', ',', '▁he', '▁should', '▁take', '▁it', '▁out', '▁and', '▁point', '▁it', '▁at', '▁the', '▁nearest', '▁person', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁when', '▁you', '▁take', '▁a', '▁gun', '▁out', ',', '▁the', '▁sound', '▁of', '▁the', '▁safety', '▁being', '▁taken', '▁off', '▁will', '▁be', '▁made', ',', '▁even', '▁if', '▁your', '▁finger', '▁is', '▁nowhere', '▁near', '▁the', '▁safety', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁reporters', '▁agree', '▁to', '▁go', '▁half', '-', 'way', '▁around', '▁the', '▁world', '▁in', '▁order', '▁to', '▁expose', '▁something', '▁', '-', '▁without', '▁having', '▁the', '▁faint', 'est', '▁idea', '▁what', '▁they', \"'\", 're', '▁exposing', '.', '▁background', '▁research', '▁and', '▁vague', '▁knowledge', '▁are', '▁out', '▁of', '▁fashion', '▁in', '▁modern', '▁journalism', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁handgun', 's', '▁hold', '▁at', '▁least', '▁52', '▁bullets', '▁in', '▁one', '▁clip', ',', '▁and', '▁then', '▁more', '▁than', '▁that', '▁in', '▁the', '▁next', '▁clip', '.', '▁despite', '▁that', ',', '▁those', '▁with', '▁guns', '▁claim', '▁that', '▁they', '▁will', '▁need', '▁more', '▁am', 'mo', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁expensive', '▁cameras', '▁', '(', 'also', ',', '▁remember', '▁that', '▁the', '▁reporter', '▁only', '▁has', '▁one', '▁camera', ')', '▁are', '▁regularly', '▁left', '▁behind', '▁without', '▁even', '▁a', '▁moment', \"'\", 's', '▁hesitation', '▁or', '▁regret', '.', '▁these', '▁cameras', '▁amazing', 'ly', '▁manage', '▁to', '▁make', '▁their', '▁way', '▁back', '▁to', '▁the', '▁reporter', '▁all', '▁by', '▁themselves', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁the', '▁blonde', '▁girl', '▁really', '▁is', '▁the', '▁stupid', '▁one', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁the', '▁same', '▁girl', '▁that', '▁says', '▁not', '▁to', '▁go', '▁into', '▁a', '▁house', '▁because', '▁a', '▁', 'kom', 'odo', '▁dragon', '▁can', '▁easily', '▁run', '▁right', '▁through', '▁it', ',', '▁thus', '▁making', '▁it', '▁unsafe', ',', '▁takes', '▁a', '▁team', '▁into', '▁a', '▁building', '▁made', '▁of', '▁the', '▁same', '▁material', '▁for', '▁protection', '▁', '-', '▁and', '▁nobody', '▁says', '▁a', '▁word', '▁about', '▁it', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁high', '-', 'tech', '▁facilities', '▁look', '▁like', '▁simple', '▁offices', '▁with', '▁high', '▁school', '▁chemistry', '▁sets', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁genetically', '-', 'mod', 'ified', '▁snakes', '▁grow', '▁from', '▁normal', '▁size', '▁to', '▁100', '▁feet', '▁long', '▁in', '▁a', '▁matter', '▁of', '▁a', '▁day', ',', '▁but', '▁don', \"'\", 't', '▁grow', '▁at', '▁all', '▁in', '▁the', '▁weeks', '▁either', '▁side', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁the', '▁military', '▁routinely', '▁destroy', 's', '▁entire', '▁islands', '▁when', '▁people', '▁don', \"'\", 't', '▁meet', '▁contact', '▁deadline', 's', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁men', '▁with', '▁guns', '▁don', \"'\", 't', '▁necessarily', '▁change', '▁the', '▁direction', '▁they', \"'\", 're', '▁shooting', '▁when', '▁their', '▁target', '▁is', '▁no', '▁longer', '▁right', '▁in', '▁front', '▁of', '▁them', '.', '▁instead', ',', '▁they', '▁just', '▁keep', '▁shooting', '▁into', '▁the', '▁air', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁the', '▁better', '▁looking', '▁you', '▁are', ',', '▁the', '▁greater', '▁your', '▁chance', '▁of', '▁surviving', '▁giant', '▁creatures', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁women', \"'\", 's', '▁intuition', '▁is', '▁reliable', '▁enough', '▁to', '▁change', '▁even', '▁the', '▁most', '▁stubborn', '▁of', '▁minds', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁any', '▁time', '▁you', \"'\", 're', '▁being', '▁hunted', '▁by', '▁giant', '▁creatures', '▁is', '▁a', '▁great', '▁time', '▁to', '▁hit', '▁on', '▁girls', '▁half', '▁your', '▁age', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁animal', '▁noise', 's', '▁are', '▁an', '▁appropriate', '▁mask', 'ing', '▁noise', '▁for', '▁', \"'\", 's', 'wear', 'ing', \"'\", '▁at', '▁the', '▁same', '▁volume', '.', '<', 'br', '▁', '/', '>', '<', 'br', '▁', '/', '>', '-', '▁old', '▁is', 'ra', 'eli', '▁and', '▁', 'russian', '▁planes', '▁are', '▁regularly', '▁used', '▁by', '▁the', '▁us', '▁military', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs :  [52, 27, 115, 24, 17, 3929, 2666, 21704, 19, 502, 29, 81, 19530, 20, 2609, 27, 102, 4668, 9, 634, 19, 193, 41, 24, 274, 1312, 29, 1734, 22, 47, 72, 2245, 6832, 181, 18, 4251, 22, 468, 60, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 22008, 12318, 287, 645, 2934, 17, 20964, 12200, 8984, 23, 9, 36, 855, 26, 46, 918, 160, 142, 537, 44, 5366, 38, 18, 17, 20964, 12200, 19, 12318, 125, 282, 26, 46, 216, 479, 36, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 252, 162, 22, 1685, 163, 12434, 37, 24, 2934, 830, 3890, 19, 49, 24, 2934, 17, 20964, 12200, 8984, 19, 27, 125, 22, 1434, 105, 9, 18, 5712, 22, 52, 1614, 27, 108, 44, 26, 189, 72, 258, 22, 1078, 172, 194, 19, 25, 59, 363, 44, 170, 446, 177, 19, 259, 18, 17, 20964, 12200, 27, 203, 244, 22, 44, 19, 21, 137, 44, 170, 1434, 105, 19, 7364, 2744, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 641, 20, 15981, 4546, 2208, 2974, 8803, 583, 18, 7095, 343, 862, 28, 14896, 2441, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 19774, 27, 24, 2429, 14894, 2827, 19, 21, 48, 15877, 7244, 28, 2334, 73, 2694, 56, 3480, 167, 17225, 1771, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 340, 20, 6799, 1255, 2043, 220, 26, 46, 197, 2934, 17, 20964, 12200, 8984, 23, 834, 259, 63, 41, 364, 65, 17, 12433, 20, 18, 95, 13, 5206, 163, 23, 9, 2163, 18, 920, 13, 25585, 1697, 20, 166, 8984, 23, 51, 359, 22, 112, 33, 36, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 13685, 24, 546, 777, 13656, 31, 19962, 2501, 2870, 2677, 19, 24, 5602, 21, 24, 3023, 249, 33, 65, 3023, 27, 71, 18, 5343, 21, 3271, 44, 53, 214, 97, 1224, 5343, 19, 24, 205, 3023, 19, 15043, 23, 1813, 41, 71, 2653, 10524, 1716, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 44, 1388, 24, 6037, 2870, 9601, 19, 21, 65, 601, 51, 24, 2363, 19, 43, 170, 182, 36, 78, 21, 424, 36, 38, 18, 9646, 601, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 44, 182, 24, 2363, 78, 19, 18, 1224, 20, 18, 1445, 163, 572, 177, 53, 39, 140, 19, 176, 108, 73, 4586, 27, 8994, 479, 18, 1445, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 1891, 2125, 22, 216, 455, 13, 1550, 199, 18, 185, 25, 374, 22, 12649, 359, 17, 13, 286, 491, 18, 9545, 1277, 875, 113, 63, 26, 88, 19962, 9, 3002, 557, 21, 14446, 1556, 41, 78, 20, 3082, 25, 1380, 11931, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 22008, 23, 859, 38, 390, 5085, 12318, 25, 65, 9555, 19, 21, 137, 70, 100, 29, 25, 18, 244, 9555, 9, 1433, 29, 19, 186, 33, 4389, 1857, 29, 63, 53, 214, 70, 569, 1495, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 3155, 6414, 17, 10, 6951, 19, 1633, 29, 18, 5602, 114, 51, 65, 3023, 11, 41, 3955, 263, 583, 286, 176, 24, 1070, 26, 23, 21955, 49, 6595, 9, 166, 6414, 3704, 111, 3300, 22, 144, 58, 162, 126, 22, 18, 5602, 71, 37, 1038, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 17478, 1615, 343, 27, 18, 6179, 65, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 219, 1615, 29, 349, 50, 22, 216, 91, 24, 480, 149, 24, 17, 20964, 12200, 8984, 64, 1659, 446, 203, 135, 36, 19, 2189, 441, 36, 22176, 19, 1321, 24, 230, 91, 24, 540, 140, 20, 18, 219, 1270, 28, 1754, 17, 13, 21, 6673, 349, 24, 1139, 75, 36, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 227, 13, 4906, 2110, 338, 115, 1369, 3322, 33, 227, 297, 10421, 3321, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18782, 13, 7124, 4452, 20751, 2220, 40, 1791, 1218, 22, 842, 1108, 206, 25, 24, 918, 20, 24, 191, 19, 57, 220, 26, 46, 2220, 38, 71, 25, 18, 754, 725, 366, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 370, 13183, 5084, 23, 1253, 5852, 90, 104, 220, 26, 46, 767, 1056, 4861, 23, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 416, 33, 4389, 220, 26, 46, 5465, 459, 18, 2070, 63, 26, 88, 2601, 90, 58, 1983, 27, 116, 996, 203, 25, 605, 20, 107, 9, 1129, 19, 63, 125, 435, 2601, 91, 18, 562, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 352, 589, 44, 41, 19, 18, 1786, 73, 1116, 20, 9330, 2934, 9737, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 412, 26, 23, 27069, 27, 5401, 456, 22, 459, 176, 18, 127, 15806, 20, 7927, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 124, 92, 44, 26, 88, 163, 23064, 37, 2934, 9737, 27, 24, 312, 92, 22, 645, 31, 2537, 455, 73, 679, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 2870, 4612, 23, 41, 48, 2183, 9722, 56, 4612, 28, 17, 26, 23, 10727, 56, 26, 38, 18, 219, 2961, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 532, 27, 840, 8252, 21, 17, 23975, 5698, 41, 3955, 179, 37, 18, 211, 370, 9, 4145, 23, 3882, 3158, 4145, 11974, 23, 3158]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AlbertTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "if bert_model == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif bert_model == 'albert':\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
    "elif bert_model == 'roberta':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "elif bert_model == 'xlnet':\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "print('Original : ', sentences[0])\n",
    "print('Tokenized : ', tokenizer.tokenize(sentences[0]))\n",
    "print('Token IDs : ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sL40H1W1AYoK"
   },
   "source": [
    "Sentence to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "u2m4kI1kAYoL",
    "outputId": "c8d3e81e-d7bb-403d-a537-b143c4ee0a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  This is like a zoology textbook, given that its depiction of animals is so accurate. However, here are a few details that appear to have been slightly modified during the transition to film:<br /><br />- Handgun bullets never hit giant Komodo dragons. It doesn't matter how many times you shoot at the Komodo, bullets just won't go near it.<br /><br />- The best way to avoid being eaten by a giant Cobra, or a giant Komodo dragon, is just to stand there. The exception to this rule is if you've been told to stay very still, in which case you should run off, until the Komodo is right next to you, and then you should stand there, expecting defeat.<br /><br />- Minutes of choppy slow motion footage behind the credits really makes for enjoyable watching.<br /><br />- $5,000 is a memory enhancement tool, and an ample substitute for losing your boating license/getting arrested.<br /><br />- Members of elite army units don't see giant Komodo dragons coming until they are within one metre of the over-sized beings. Maybe the computer-generated nature of these dragons has something to do with it.<br /><br />- When filming a news story aiming on exposing illegal animal testing, a reporter and a cameraman with one camera is all the gear and personnel you will need; sound gear, a second camera, microphones etc are all superfluous.<br /><br />- When you hear a loud animal scream, and one person has a gun, he should take it out and point it at the nearest person.<br /><br />- When you take a gun out, the sound of the safety being taken off will be made, even if your finger is nowhere near the safety<br /><br />- Reporters agree to go half-way around the world in order to expose something - without having the faintest idea what they're exposing. Background research and vague knowledge are out of fashion in modern journalism.<br /><br />- Handguns hold at least 52 bullets in one clip, and then more than that in the next clip. Despite that, those with guns claim that they will need more ammo.<br /><br />- Expensive cameras (also, remember that the reporter only has one camera) are regularly left behind without even a moment's hesitation or regret. These cameras amazingly manage to make their way back to the reporter all by themselves.<br /><br />- The blonde girl really is the stupid one.<br /><br />- The same girl that says not to go into a house because a Komodo dragon can easily run right through it, thus making it unsafe, takes a team into a building made of the same material for protection - and nobody says a word about it.<br /><br />- High-tech facilities look like simple offices with high school chemistry sets.<br /><br />- Genetically-modified snakes grow from normal size to 100 feet long in a matter of a day, but don't grow at all in the weeks either side.<br /><br />- The military routinely destroys entire islands when people don't meet contact deadlines.<br /><br />- Men with guns don't necessarily change the direction they're shooting when their target is no longer right in front of them. Instead, they just keep shooting into the air.<br /><br />- The better looking you are, the greater your chance of surviving giant creatures.<br /><br />- Women's intuition is reliable enough to change even the most stubborn of minds.<br /><br />- Any time you're being hunted by giant creatures is a great time to hit on girls half your age.<br /><br />- Animal noises are an appropriate masking noise for 'swearing' at the same volume.<br /><br />- Old Israeli and Russian planes are regularly used by the US Military. [SEP] [CLS]\n",
      "id:  [52, 27, 115, 24, 17, 3929, 2666, 21704, 19, 502, 29, 81, 19530, 20, 2609, 27, 102, 4668, 9, 634, 19, 193, 41, 24, 274, 1312, 29, 1734, 22, 47, 72, 2245, 6832, 181, 18, 4251, 22, 468, 60, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 22008, 12318, 287, 645, 2934, 17, 20964, 12200, 8984, 23, 9, 36, 855, 26, 46, 918, 160, 142, 537, 44, 5366, 38, 18, 17, 20964, 12200, 19, 12318, 125, 282, 26, 46, 216, 479, 36, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 18, 252, 162, 22, 1685, 163, 12434, 37, 24, 2934, 830, 3890, 19, 49, 24, 2934, 17, 20964, 12200, 8984, 19, 27, 125, 22, 1434, 105, 9, 18, 5712, 22, 52, 1614, 27, 108, 44, 26, 189, 72, 258, 22, 1078, 172, 194, 19, 25, 59, 363, 44, 170, 446, 177, 19, 259, 18, 17, 20964, 12200, 27, 203, 244, 22, 44, 19, 21, 137, 44, 170, 1434, 105, 19, 7364, 2744, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 641, 20, 15981, 4546, 2208, 2974, 8803, 583, 18, 7095, 343, 862, 28, 14896, 2441, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 19774, 27, 24, 2429, 14894, 2827, 19, 21, 48, 15877, 7244, 28, 2334, 73, 2694, 56, 3480, 167, 17225, 1771, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 340, 20, 6799, 1255, 2043, 220, 26, 46, 197, 2934, 17, 20964, 12200, 8984, 23, 834, 259, 63, 41, 364, 65, 17, 12433, 20, 18, 95, 13, 5206, 163, 23, 9, 2163, 18, 920, 13, 25585, 1697, 20, 166, 8984, 23, 51, 359, 22, 112, 33, 36, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 13685, 24, 546, 777, 13656, 31, 19962, 2501, 2870, 2677, 19, 24, 5602, 21, 24, 3023, 249, 33, 65, 3023, 27, 71, 18, 5343, 21, 3271, 44, 53, 214, 97, 1224, 5343, 19, 24, 205, 3023, 19, 15043, 23, 1813, 41, 71, 2653, 10524, 1716, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 44, 1388, 24, 6037, 2870, 9601, 19, 21, 65, 601, 51, 24, 2363, 19, 43, 170, 182, 36, 78, 21, 424, 36, 38, 18, 9646, 601, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 90, 44, 182, 24, 2363, 78, 19, 18, 1224, 20, 18, 1445, 163, 572, 177, 53, 39, 140, 19, 176, 108, 73, 4586, 27, 8994, 479, 18, 1445, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 1891, 2125, 22, 216, 455, 13, 1550, 199, 18, 185, 25, 374, 22, 12649, 359, 17, 13, 286, 491, 18, 9545, 1277, 875, 113, 63, 26, 88, 19962, 9, 3002, 557, 21, 14446, 1556, 41, 78, 20, 3082, 25, 1380, 11931, 9, 6461, 2369, 17, 167, 3151, 6461, 2369, 17, 167, 3151, 13, 22008, 23, 859, 38, 390, 5085, 12318, 25, 65, 9555, 19, 21, 137, 70, 100, 29, 25, 18, 244, 9555, 9, 1433, 4, 3]\n",
      "Max sentence length:  512\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "\n",
    "for s in sentences:\n",
    "    encoded_sentence = tokenizer.encode(s, max_length = 512, add_special_tokens=True)\n",
    "    input_ids.append(encoded_sentence)\n",
    "    \n",
    "print('original: ', sentences[0])\n",
    "print('id: ', input_ids[0])\n",
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkHGEfJhAYoN"
   },
   "source": [
    "Add padding and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvnwuYlZAYoO"
   },
   "outputs": [],
   "source": [
    "def add_padding_and_truncate(input_ids):\n",
    "    MAX_LEN = 64\n",
    "    for index, input_id in enumerate(input_ids):\n",
    "        for i in range(MAX_LEN - len(input_id)):\n",
    "          input_id.insert(0, 0)\n",
    "        if len(input_id) > MAX_LEN:\n",
    "          input_ids[index] = input_id[:MAX_LEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dHoWz1rtAYoQ",
    "outputId": "24bbf0fa-3338-40ef-a254-705981638e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After max question length:  64\n"
     ]
    }
   ],
   "source": [
    "# Fit sentence's length to MAX_LEN\n",
    "add_padding_and_truncate(input_ids)\n",
    "\n",
    "print('After max question length: ', max([len(id) for id in input_ids]))\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ca1FnQVUAYoS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.1)\n",
    "train_masks, valid_masks, _, _ = train_test_split(attention_masks, labels, random_state=2020, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "valid_inputs = torch.tensor(valid_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "valid_masks = torch.tensor(valid_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YeciXKeAYoX"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "valid_sampler = RandomSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyvHK7gbAYoZ"
   },
   "source": [
    "# Training\n",
    "\n",
    "BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpfD-vUfAYoa"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from transformers import AlbertForSequenceClassification, AlbertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetConfig\n",
    "\n",
    "num_label = 2 # depends on data.\n",
    "\n",
    "if bert_model == 'bert':\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = num_label, \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "elif bert_model == 'albert':\n",
    "    model = AlbertForSequenceClassification.from_pretrained(\n",
    "        'albert-base-v2',\n",
    "        num_labels = num_label,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "elif bert_model == 'roberta':\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        num_labels = num_label,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "elif bert_model == 'xlnet':\n",
    "    model = XLNetForSequenceClassification.from_pretrained(\n",
    "        'xlnet-base-cased',\n",
    "        num_labels = num_label,\n",
    "    )\n",
    "\n",
    "if device.type == 'cuda':\n",
    "  model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOPm2ntbAYof"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlsy2adMAYoi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kJa6xacAYok"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1ErScfHqAYom",
    "outputId": "e88b3a6c-bfbe-4dea-a964-23759d4f2d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingo\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    113.    Elapsed: 0:00:36.\n",
      "  Batch    80  of    113.    Elapsed: 0:01:12.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epcoh took: 0:01:41\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    113.    Elapsed: 0:00:36.\n",
      "  Batch    80  of    113.    Elapsed: 0:01:12.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epcoh took: 0:01:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    113.    Elapsed: 0:00:38.\n",
      "  Batch    80  of    113.    Elapsed: 0:01:16.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epcoh took: 0:01:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    113.    Elapsed: 0:00:38.\n",
      "  Batch    80  of    113.    Elapsed: 0:01:17.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epcoh took: 0:01:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in valid_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAkvLl9pAYoo"
   },
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Cmcj6mPBAYop",
    "outputId": "17a43dc1-1730-49f8-f6b6-52c2ac5eac88"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_values, 'b-o')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vyWFO7rAYor"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "ZPBuLiXxAYos",
    "outputId": "e7ea777d-05ca-4add-f399-a1ed96585c05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingo\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.831\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./Dataset/IMDB_Dataset.csv\", delimiter=',', header=None, names=['review', 'sentiment'])\n",
    "sentences = df.review.values[1:1000]\n",
    "labels = df.sentiment.values[1:1000]\n",
    "labels = [1 if l == 'positive' else 0 for l in labels]\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"./Dataset/ag_news_test.csv\", delimiter=',', header=None, names=['category', \"head\", 'content'])\n",
    "\n",
    "# Print number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "sentences = df.content.values\n",
    "labels = df.category.values - 1\n",
    "\"\"\"\n",
    "\n",
    "if bert_model == 'xlnet':\n",
    "    sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "\n",
    "input_ids = []\n",
    "\n",
    "for s in sentences:\n",
    "    encoded_sentence = tokenizer.encode(s, max_length = 512, add_special_tokens=True)\n",
    "    input_ids.append(encoded_sentence)\n",
    "        \n",
    "add_padding_and_truncate(input_ids)\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    attention_masks.append(att_mask)\n",
    "    \n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_accuracy = 0\n",
    "eval_steps = 0\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    #predictions.append(logits)\n",
    "    #true_labels.append(label_ids)\n",
    "    \n",
    "    eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    eval_steps += 1\n",
    "\n",
    "print(\"Accuracy: {0:.3f}\".format(eval_accuracy/eval_steps))\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intent_Classification_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "29ab35ebeb04449c97bfff988caaeb57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9250a549d7fd40ad93d2cdd12fb6f91f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4aa805f33c044f58c4d0dc5611364e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad81993d700646b497f8fbcf2423fe15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4aa805f33c044f58c4d0dc5611364e1",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af99d4addbca4e56970fb2a84d59ed74",
      "value": 231508
     }
    },
    "af99d4addbca4e56970fb2a84d59ed74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cb136730dc874ddb88ff884956c373c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f209950dc361472b88dca6a52006ea59",
      "placeholder": "​",
      "style": "IPY_MODEL_29ab35ebeb04449c97bfff988caaeb57",
      "value": " 232k/232k [00:00&lt;00:00, 990kB/s]"
     }
    },
    "f05b76c973f249519c510b7aade21510": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad81993d700646b497f8fbcf2423fe15",
       "IPY_MODEL_cb136730dc874ddb88ff884956c373c8"
      ],
      "layout": "IPY_MODEL_9250a549d7fd40ad93d2cdd12fb6f91f"
     }
    },
    "f209950dc361472b88dca6a52006ea59": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
